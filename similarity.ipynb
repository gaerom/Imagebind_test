{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 수정된 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "from imagebind import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, audio_dir, image_dir, text_file, transform=None):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.image_dir = image_dir\n",
    "        self.text_file = text_file\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.audio_files = sorted([f for f in os.listdir(audio_dir) if f.endswith('.wav')])\n",
    "        self.image_files = [f.replace('.wav', '.png') for f in self.audio_files]\n",
    "        \n",
    "        with open(text_file, 'r') as f:\n",
    "            self.texts = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_filename = self.audio_files[idx]\n",
    "        image_filename = self.image_files[idx]\n",
    "        text = self.texts[idx]  \n",
    "        \n",
    "        audio_path = os.path.join(self.audio_dir, audio_filename)\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "        \n",
    "        audio_data = data.load_and_transform_audio_data([audio_path], device)\n",
    "        image_data = data.load_and_transform_vision_data([image_path], device)\n",
    "        text_data = data.load_and_transform_text([text], device)\n",
    "\n",
    "        if image_data.ndim > 4:  # Assuming that image_data can be a 5D tensor [1, 3, 1, H, W]\n",
    "            image_data = image_data.squeeze(2)\n",
    "        \n",
    "        return audio_data, image_data, text_data, (audio_filename, image_filename, text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 각 modality에 해당하는 data 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = imagebind_model.imagebind_huge(pretrained=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "audio_dir = './modalities/audios'\n",
    "image_dir = './modalities/frames_test' # for test\n",
    "text_file = './modalities/labels.txt' # for test\n",
    "\n",
    "dataset = MultimodalDataset(audio_dir, image_dir, text_file)\n",
    "# dataloader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files: ('dog growling_32720.wav', 'dog growling_32720.png', 'dog growling')\n",
      "Vision x Text similarity:  tensor([[30.6504]], device='cuda:0')\n",
      "Audio x Text similarity:  tensor([[649.1145]], device='cuda:0')\n",
      "Vision x Audio similarity:  tensor([[9.3685]], device='cuda:0')\n",
      "----------------------------------------------------------\n",
      "\n",
      "Files: ('male singing_27393.wav', 'male singing_27393.png', 'male singing')\n",
      "Vision x Text similarity:  tensor([[26.5837]], device='cuda:0')\n",
      "Audio x Text similarity:  tensor([[363.7698]], device='cuda:0')\n",
      "Vision x Audio similarity:  tensor([[8.2802]], device='cuda:0')\n",
      "----------------------------------------------------------\n",
      "\n",
      "Files: ('people babbling_88641.wav', 'people babbling_88641.png', 'people babbling')\n",
      "Vision x Text similarity:  tensor([[18.0223]], device='cuda:0')\n",
      "Audio x Text similarity:  tensor([[162.3833]], device='cuda:0')\n",
      "Vision x Audio similarity:  tensor([[6.6675]], device='cuda:0')\n",
      "----------------------------------------------------------\n",
      "\n",
      "Files: ('police car (siren)_95560.wav', 'police car (siren)_95560.png', 'police car (siren)')\n",
      "Vision x Text similarity:  tensor([[23.2436]], device='cuda:0')\n",
      "Audio x Text similarity:  tensor([[549.5454]], device='cuda:0')\n",
      "Vision x Audio similarity:  tensor([[6.5031]], device='cuda:0')\n",
      "----------------------------------------------------------\n",
      "\n",
      "Files: ('yodelling_124652.wav', 'yodelling_124652.png', 'yodelling')\n",
      "Vision x Text similarity:  tensor([[31.6630]], device='cuda:0')\n",
      "Audio x Text similarity:  tensor([[457.7261]], device='cuda:0')\n",
      "Vision x Audio similarity:  tensor([[8.1421]], device='cuda:0')\n",
      "----------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vt_sims = []\n",
    "at_sims = []\n",
    "va_sims = []\n",
    "\n",
    "filenames_list = []\n",
    "\n",
    "for audio_batch, image_batch, text_batch, filenames in dataset:\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        inputs = {\n",
    "            ModalityType.AUDIO: audio_batch,\n",
    "            ModalityType.VISION: image_batch,\n",
    "            ModalityType.TEXT: text_batch\n",
    "        }\n",
    "        \n",
    "        embeddings = model(inputs)\n",
    "        \n",
    "        vt_sim = embeddings[ModalityType.VISION] @ embeddings[ModalityType.TEXT].T\n",
    "        at_sim = embeddings[ModalityType.AUDIO] @ embeddings[ModalityType.TEXT].T\n",
    "        va_sim = embeddings[ModalityType.VISION] @ embeddings[ModalityType.AUDIO].T\n",
    "        \n",
    "        # GPU tensor를 CPU로 이동\n",
    "        vt_sims.append(vt_sim.cpu().numpy())\n",
    "        at_sims.append(at_sim.cpu().numpy())\n",
    "        va_sims.append(va_sim.cpu().numpy())\n",
    "        \n",
    "        for _ in range(len(vt_sim)):\n",
    "            filenames_list.append(filenames)\n",
    "        \n",
    "        \n",
    "        print(f'Files: {filenames}') # modality pair들이 알맞게 들어가는지 확인 가능\n",
    "        print('Vision x Text similarity: ', vt_sim)\n",
    "        print('Audio x Text similarity: ', at_sim)\n",
    "        print('Vision x Audio similarity: ', va_sim)\n",
    "        print('----------------------------------------------------------\\n')\n",
    "        \n",
    "vt_sims = np.concatenate(vt_sims, axis=None)\n",
    "at_sims = np.concatenate(at_sims, axis=None)\n",
    "va_sims = np.concatenate(va_sims, axis=None)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.650362 26.583687 18.02227  23.24356  31.663048]\n"
     ]
    }
   ],
   "source": [
    "# print(vt_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 similarity의 mean, median, max 출력\n",
    "# def statistics(data):\n",
    "#     mean_val = np.mean(data)\n",
    "#     median_val = np.median(data)\n",
    "#     max_val = np.max(data)\n",
    "    \n",
    "#     print(f'Mean: {mean_val}, Median: {median_val}, Max: {max_val}')\n",
    "    \n",
    "# print('Vision x Text'), statistics(vt_sims)\n",
    "# print('Audio x Text'), statistics(at_sims)\n",
    "# print('Vision x Audio'), statistics(va_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual-text median: 26.58368682861328\n",
      "audio-text median: 457.72607421875\n",
      "visual-audio median: 8.142138481140137\n",
      "('people babbling_88641.wav', 'people babbling_88641.png', 'people babbling')\n"
     ]
    }
   ],
   "source": [
    "# 3개의 similarity -> 산술평균은 부적절\n",
    "# 3개의 similarity의 median 값을 모두 넘지 못하는 data는 제거(로 일단 구현)\n",
    "vt_median = np.median(vt_sims)\n",
    "at_median = np.median(at_sims)\n",
    "va_median = np.median(va_sims)\n",
    "\n",
    "\n",
    "print(f'visual-text median: {vt_median}')\n",
    "print(f'audio-text median: {at_median}')\n",
    "print(f'visual-audio median: {va_median}')\n",
    "\n",
    "# for i, (vt, at, va) in enumerate(zip(vt_sims, at_sims, va_sims)):\n",
    "#     if vt < vt_median and at < at_median and va < va_median: # 모두 넘지 못하는 경우\n",
    "#         print(filenames_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people babbling_88641.wav\n"
     ]
    }
   ],
   "source": [
    "# 제거하고 싶은 audio를 전체 dataset에서 제거하기 위해, 해당 audio 파일명만 txt에 저장\n",
    "\n",
    "with open('/mnt/storage1/vggsoundsync/below_median_test.txt', 'w') as file: # 경로 수정\n",
    "    for i, (vt, at, va) in enumerate(zip(vt_sims, at_sims, va_sims)):\n",
    "        if vt < vt_median and at < at_median and va < va_median: # 모두 넘지 못하는 경우\n",
    "            audio_filename = filenames_list[i][0] \n",
    "            file.write(f\"{audio_filename}\\n\")\n",
    "            print(audio_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 위에서 저장한 txt(제거 대상)를 이용해 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 알아낸 audio들만 다른 dir로 이동시키기 (완전 삭제는 X)\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "source_dir = '/mnt/storage1/trainvideo_10_audios' # 경로 확인\n",
    "target_dir = 'below'\n",
    "\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    " \n",
    "with open('below_median_test.txt', 'r') as file: # 경로 확인\n",
    "    for line in file:\n",
    "        filename = line.strip()\n",
    "\n",
    "        source_file = os.path.join(source_dir, filename)\n",
    "        target_file = os.path.join(target_dir, filename)\n",
    "        \n",
    "        if os.path.exists(source_file):\n",
    "            shutil.move(source_file, target_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imagebind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
